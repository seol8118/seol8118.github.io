---
title: "Le-Net"
date: 2020-08-20
category:
  - CNN
tag :
  - CNN
sidebar:
  nav: sidebar-cnn
mathjax: "true"
author_profile: false
toc: true
toc_label: "Contents"
toc_icon: "cog"
toc_sticky: true
header:
  overlay_image: /assets/images/dataScience.jpg
  overlay_filter: 0.5
comments: true  
---
 
> ML study > CNN architecture > LeNet

<script type="text/javascript" 
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>

## 1. LeNet-1

<center><img src="/assets/images/cnn/lenet1.jpg" width="70%"  ></center>

[<center>Source</center>](http://yann.lecun.com/exdb/publis/pdf/lecun-95b.pdf)

- Yann LeCun을 비롯한 연구원들은 이미 위와 같은 CNN network 를 1990년에 발표하였다. 그림은 LeNet-1의 구조를 나타내며 convolution과 sub sampling (pooling) 과정을 거쳐 classification 하는 구조로 현재 사용되는 network 와 굉장히 유사하다.

- 작은 Input size와 적은 개수의 feature map을 사용했음에도 불구하고 test set 에서 1.7%의 에러율을 얻었다고 한다. 

## 2. LeNet-5

<center><img src="/assets/images/cnn/lenet2.jpg" width="70%"  ></center>

[<center>Source</center>](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

- LeNet-1 으로부터 시간이 지나 컴퓨팅 능력이 개선되어 LeNet-5에서는 32x32의 조금 더 큰 Input을 사용했고 featur map 개수와 fully connected layer 수를 늘려 정확도를 개선하였다.

- LeNet-5 는 LeNet-1 에 비해 parameter가 20배 가까이 많지만 성능면에서는 test set에서 0.95%의 에러율을 얻으며 크게 개선시켰다.

- 추가적으로 이 당시에는 activation function으로 Tanh 와 pooling 방식으로는 average pooling 방식을 사용했으며 최근 사용되는 ReLU와 max pooling layer로 network를 구성했다면 더 나은 결과를 보였을 것이다.


## Reference
\[1]: [LeNet-1](http://yann.lecun.com/exdb/publis/pdf/lecun-95b.pdf)

\[2]: [LeNet-5](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

\[3]: [LeNet 정리(Medium)](https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17)

\[4]: [LeNet 정리(Naver)](https://blog.naver.com/laonple/220648539191)