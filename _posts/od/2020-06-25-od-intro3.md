---
title: "Object Detection Evaluation Criteria"
date: 2020-06-25
category:
  - object detection
tag :
  - object detection
sidebar:
  nav: sidebar-odBackground
mathjax: "true"
author_profile: false
toc: true
toc_label: "Contents"
toc_icon: "cog"
toc_sticky: true
header:
  overlay_image: /assets/images/dataScience.jpg
  overlay_filter: 0.5
comments: true
---
<script type="text/javascript" 
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
MathJax.Hub.Config({
    displayAlign: "center"
});
</script>

> ML study > Object Detection > Background

# 1. Evaluation Criteria
- Object detection의 성능을 평가하는 요소는 Frame Per Second (FPS), precision, and recall 이 있다.


$$Precision= \frac{TP}{TP+FP}$$ 

$$Recall = \frac{TP}{TP+FN}$$

*TP: 검출했는데 잘한 것

*FP: 검출했는데 잘못한 것

*FN: 검출하지 않았는데 잘못한 것 --> 검출했어야 한 것


- Precision 과 Recall은 볼때마다 헷갈리므로 위 수식을 보고 remind 하자. Precision은 맞다고 예측한 결과 중에 실제로 맞은 값의 비율, Recall은 실제 정답인 수 중에 제대로 예측한 값의 비율이라고 생각하면 된다.

- 하나의 class에 대해서 precision과 recall은 confidence threshold 값에 따라 달라질 수 있기 때문에 여러개의 (precision, recall) 쌍을 구할 수 있다. 이 때 precision-recall curve를 그리고 아래 면적을 구한 값이 Average Precision (AP) 로 정의되며 모든 class에 대해 측정하여 평균한 mean AP (mAP) 로 부터 모델의 전반적인 성능을 평가할 수 있다.

- 그렇다면 object detection에서 True Positive(TP)는 어떻게 정의되는지 살펴보자.

Test image $$I$$ 에 대하여 {($$b_j$$,$$c_j$$,$$p_j$$)} 를 예측했다고 하자. (b는 bounding box, c는 predict한 category, p는 confidence로 정의 된다.)

이 때, TP는 {($$b_j$$,$$c_j$$,$$p_j$$)} 가 다음과 같은 2가지 조건을 만족했을 때 성립한다.

1. $$c_j$$ = $$c_g$$
2. IOU > (특정값)

1번은 클래스를 맞게 예측했는지를 의미하며 2번에서 IOU는 bounding box의 예측영역과 실제영역의 교집합 을 합집합으로 나눈 값으로 정확히 예측했을때에 1이 된다. 일반적으로 0.5 이상이 되었을 때에 잘 예측했다고 판단한다.

- 이렇게 얻은 TP, FP, FN 로 부터 최종적으로 mAP를 계산할 수 있게 된다.

- 다음은 이 알고리즘을 자세하게 나타낸 그림이다.

<center><img src="/assets/images/od/survey10.jpg" ></center>



## Reference
\[1]: [Deep Learning for Generic Object Detection: A Survey](https://doi.org/10.1007/s11263-019-01247-4)

\[2]: [AP 정의](https://hyeonnii.tistory.com/284)


<br><br>
